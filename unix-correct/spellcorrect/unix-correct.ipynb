{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06178f6b",
   "metadata": {},
   "source": [
    "# Functional Spell Correction Demo (Norvig Style)\n",
    "\n",
    "This notebook demonstrates a functional approach to spell correction using:\n",
    "- N-gram Language Models with Laplace smoothing\n",
    "- Noisy Channel Model\n",
    "- Error Confusion Matrices\n",
    "- Damerau-Levenshtein Edit Distance\n",
    "\n",
    "We'll build up from basic components to a complete spell checker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dff5c67",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de02dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75e0080",
   "metadata": {},
   "source": [
    "## 2. Corpus Loading\n",
    "\n",
    "First, let's load and process a corpus of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed8118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(filename='corpus.data'):\n",
    "    \"\"\"Load and tokenize corpus from file.\"\"\"\n",
    "    print(f\"Loading Corpus from {filename}\")\n",
    "    with open(filename, 'r') as f:\n",
    "        corpus = f.read()\n",
    "    print(\"Processing Corpus\")\n",
    "    return corpus.split(' ')\n",
    "\n",
    "# Demo: Load a small sample\n",
    "words = load_corpus()\n",
    "print(f\"Total words: {len(words)}\")\n",
    "print(f\"First 10 words: {words[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14002d77",
   "metadata": {},
   "source": [
    "## 3. N-gram Creation\n",
    "\n",
    "Create unigram and bigram models using Counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42e6bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unigram(words):\n",
    "    \"\"\"Create Unigram Model from word list.\"\"\"\n",
    "    print(\"Creating Unigram Model\")\n",
    "    return Counter(words)\n",
    "\n",
    "def create_bigram(words):\n",
    "    \"\"\"Create Bigram Model from word list.\"\"\"\n",
    "    print(\"Creating Bigram Model\")\n",
    "    return Counter(' '.join(words[i:i+2]) for i in range(len(words)-1))\n",
    "\n",
    "# Demo: Create models\n",
    "unigram = create_unigram(words)\n",
    "bigram = create_bigram(words)\n",
    "\n",
    "print(f\"Unique words: {len(unigram)}\")\n",
    "print(f\"Most common words: {unigram.most_common(5)}\")\n",
    "print(f\"\\nMost common bigrams: {bigram.most_common(5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75e8d1f",
   "metadata": {},
   "source": [
    "## 4. Complete N-gram Model Creation\n",
    "\n",
    "Build all n-gram models in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74987ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(gram_types=['uni', 'bi'], words=None, corpus_file='corpus.data'):\n",
    "    \"\"\"Create specified n-gram models.\"\"\"\n",
    "    if words is None:\n",
    "        words = load_corpus(corpus_file)\n",
    "    \n",
    "    models = {'words': words}\n",
    "    \n",
    "    creators = {\n",
    "        'uni': create_unigram,\n",
    "        'bi': create_bigram\n",
    "    }\n",
    "    \n",
    "    for gram_type in gram_types:\n",
    "        if gram_type in creators:\n",
    "            models[gram_type] = creators[gram_type](words)\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Demo: Create models\n",
    "models = create_ngrams(['uni', 'bi'], words=words)\n",
    "print(\"Models created:\", list(models.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c69f674",
   "metadata": {},
   "source": [
    "## 5. Probability Calculation\n",
    "\n",
    "Calculate log probabilities with Laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d691cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(word, context=\"\", gram_type='uni', models=None):\n",
    "    \"\"\"Calculate Maximum Likelihood Probability with Laplace smoothing.\"\"\"\n",
    "    if models is None:\n",
    "        raise ValueError(\"models dictionary required\")\n",
    "    \n",
    "    words = models['words']\n",
    "    vocab_size = len(models.get('uni', Counter()))\n",
    "    \n",
    "    if gram_type == 'uni':\n",
    "        return math.log((models['uni'][word] + 1) / (len(words) + vocab_size))\n",
    "    elif gram_type == 'bi':\n",
    "        bigram_count = models['bi'][context] + 1\n",
    "        unigram_count = models['uni'][word] + vocab_size\n",
    "        return math.log(bigram_count / unigram_count)\n",
    "\n",
    "# Demo: Calculate probabilities\n",
    "word_prob = probability('the', gram_type='uni', models=models)\n",
    "bigram_prob = probability('the', 'the actress', gram_type='bi', models=models)\n",
    "\n",
    "print(f\"P(the) = {math.exp(word_prob):.6f} (log: {word_prob:.4f})\")\n",
    "print(f\"P(actress|the) = {math.exp(bigram_prob):.6f} (log: {bigram_prob:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d477103",
   "metadata": {},
   "source": [
    "## 6. Sentence Probability\n",
    "\n",
    "Calculate cumulative probability for a full sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debea1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_probability(sentence, models, gram_type='uni', form='antilog'):\n",
    "    \"\"\"Calculate cumulative n-gram probability for a sentence.\"\"\"\n",
    "    words = sentence.lower().split()\n",
    "    n = len(words)\n",
    "    \n",
    "    if n < 1:\n",
    "        return 0 if form == 'log' else 1\n",
    "    \n",
    "    log_prob = 0\n",
    "    \n",
    "    if gram_type == 'uni':\n",
    "        log_prob = sum(probability(w, \"\", 'uni', models) for w in words)\n",
    "    elif gram_type == 'bi':\n",
    "        for i in range(n - 1):\n",
    "            context = f\"{words[i]} {words[i+1]}\"\n",
    "            log_prob += probability(words[i], context, 'bi', models)\n",
    "    \n",
    "    return log_prob if form == 'log' else math.exp(log_prob)\n",
    "\n",
    "# Demo: Compare sentences\n",
    "sent1 = \"the actress is brilliant\"\n",
    "sent2 = \"brilliant actress the is\"\n",
    "\n",
    "prob1 = sentence_probability(sent1, models, 'bi', 'log')\n",
    "prob2 = sentence_probability(sent2, models, 'bi', 'log')\n",
    "\n",
    "print(f\"'{sent1}': {prob1:.4f}\")\n",
    "print(f\"'{sent2}': {prob2:.4f}\")\n",
    "print(f\"\\nFirst sentence is {'more' if prob1 > prob2 else 'less'} likely!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954742e7",
   "metadata": {},
   "source": [
    "## 7. Load Confusion Matrices\n",
    "\n",
    "These matrices model common typing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b27f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_confusion_matrices():\n",
    "    \"\"\"Load all confusion matrices from data files.\"\"\"\n",
    "    print(\"Loading confusion matrices\")\n",
    "    matrices = {}\n",
    "    \n",
    "    for matrix_type in ['add', 'sub', 'rev', 'del']:\n",
    "        filename = f\"{matrix_type}confusion.data\"\n",
    "        with open(filename, 'r') as f:\n",
    "            matrices[matrix_type] = ast.literal_eval(f.read())\n",
    "    \n",
    "    return matrices\n",
    "\n",
    "# Demo: Load matrices\n",
    "matrices = load_confusion_matrices()\n",
    "print(\"Matrix types:\", list(matrices.keys()))\n",
    "print(f\"Addition matrix size: {len(matrices['add'])}\")\n",
    "print(f\"Example - 'th' added: {matrices['add'].get('th', 0)} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68a2501",
   "metadata": {},
   "source": [
    "## 8. Damerau-Levenshtein Edit Distance\n",
    "\n",
    "Measures the minimum edits needed to transform one string to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f338780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def damerau_levenshtein_distance(s1, s2):\n",
    "    \"\"\"Calculate Damerau-Levenshtein Edit Distance between two strings.\"\"\"\n",
    "    s1, s2 = '#' + s1, '#' + s2\n",
    "    m, n = len(s1), len(s2)\n",
    "    \n",
    "    D = [[0] * n for _ in range(m)]\n",
    "    for i in range(m):\n",
    "        D[i][0] = i\n",
    "    for j in range(n):\n",
    "        D[0][j] = j\n",
    "    \n",
    "    for i in range(1, m):\n",
    "        for j in range(1, n):\n",
    "            costs = [\n",
    "                D[i-1][j] + 1,\n",
    "                D[i][j-1] + 1,\n",
    "                D[i-1][j-1] + (2 if s1[i] != s2[j] else 0)\n",
    "            ]\n",
    "            \n",
    "            if i > 1 and j > 1 and s1[i] == s2[j-1] and s1[i-1] == s2[j]:\n",
    "                costs.append(D[i-2][j-2] + 1)\n",
    "            \n",
    "            D[i][j] = min(costs)\n",
    "    \n",
    "    return D[m-1][n-1]\n",
    "\n",
    "# Demo: Test edit distances\n",
    "print(f\"'actress' → 'acress': {damerau_levenshtein_distance('actress', 'acress')}\")\n",
    "print(f\"'brilliant' → 'briliant': {damerau_levenshtein_distance('brilliant', 'briliant')}\")\n",
    "print(f\"'hello' → 'helo': {damerau_levenshtein_distance('hello', 'helo')}\")\n",
    "print(f\"'actor' → 'acotr': {damerau_levenshtein_distance('actor', 'acotr')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b3f97f",
   "metadata": {},
   "source": [
    "## 9. Generate Candidate Corrections\n",
    "\n",
    "Find words within edit distance 1 of the misspelled word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87976ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates(word, words, max_distance=1):\n",
    "    \"\"\"Generate candidate corrections within edit distance threshold.\"\"\"\n",
    "    candidates = {}\n",
    "    for w in words:\n",
    "        distance = damerau_levenshtein_distance(word, w)\n",
    "        if distance <= max_distance:\n",
    "            candidates[w] = distance\n",
    "    return sorted(candidates, key=candidates.get)\n",
    "\n",
    "# Demo: Create vocabulary and find candidates\n",
    "vocab = sorted(set(words))[3246:]  # Skip common words\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "misspelled = \"acress\"\n",
    "candidates = generate_candidates(misspelled, vocab)\n",
    "print(f\"\\nCandidates for '{misspelled}': {candidates[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290e4e5d",
   "metadata": {},
   "source": [
    "## 10. Detect Edit Type\n",
    "\n",
    "Identify what kind of error was made (insertion, deletion, substitution, or transposition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6af642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_edit_type(candidate, word):\n",
    "    \"\"\"Detect the type of edit between strings.\"\"\"\n",
    "    \n",
    "    def check_edits(cand, wrd, reverse=False):\n",
    "        for i in range(min(len(wrd), len(cand)) - 1):\n",
    "            if cand[0:i+1] != wrd[0:i+1]:\n",
    "                if cand[i:] == wrd[i-1:]:\n",
    "                    correct = cand[i-1]\n",
    "                    x = cand[i-2] if i > 1 else ''\n",
    "                    w = x + correct\n",
    "                    return (\"Deletion\", correct, '', x, w)\n",
    "                elif cand[i:] == wrd[i+1:]:\n",
    "                    error = wrd[i]\n",
    "                    if i == 0:\n",
    "                        w, x = '#', '#' + error\n",
    "                    else:\n",
    "                        w, x = wrd[i-1], wrd[i-1] + error\n",
    "                    return (\"Insertion\", '', error, x, w)\n",
    "                if i + 1 < len(cand) and i + 1 < len(wrd) and cand[i+1:] == wrd[i+1:]:\n",
    "                    correct, error = cand[i], wrd[i]\n",
    "                    return (\"Substitution\", correct, error, error, correct)\n",
    "                if (i + 1 < len(wrd) and i + 2 <= len(cand) and \n",
    "                    cand[i] == wrd[i+1] and i + 2 <= len(wrd) and cand[i+2:] == wrd[i+2:]):\n",
    "                    correct = cand[i] + cand[i+1]\n",
    "                    error = wrd[i] + wrd[i+1]\n",
    "                    return (\"Reversal\", correct, error, error, correct)\n",
    "        return None\n",
    "    \n",
    "    if word == candidate:\n",
    "        return (\"None\", '', '', '', '')\n",
    "    \n",
    "    result = check_edits(candidate, word)\n",
    "    if result:\n",
    "        return result\n",
    "    \n",
    "    result = check_edits(candidate[::-1], word[::-1], reverse=True)\n",
    "    if result:\n",
    "        return result\n",
    "    \n",
    "    return (\"None\", '', '', '', '')\n",
    "\n",
    "# Demo: Detect edits\n",
    "print(\"actress → acress:\", detect_edit_type('actress', 'acress')[0])\n",
    "print(\"brilliant → briliant:\", detect_edit_type('brilliant', 'briliant')[0])\n",
    "print(\"actor → acotr:\", detect_edit_type('actor', 'acotr')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae97ad0",
   "metadata": {},
   "source": [
    "## 11. Channel Model Probability\n",
    "\n",
    "Calculate the probability that a user would make this specific error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391200c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_model_probability(x, y, edit_type, matrices, corpus):\n",
    "    \"\"\"Calculate channel model probability for errors using confusion matrices.\"\"\"\n",
    "    if edit_type == 'add':\n",
    "        if x == '#':\n",
    "            count = corpus.count(' ' + y)\n",
    "        else:\n",
    "            count = corpus.count(x)\n",
    "        return matrices['add'].get(x + y, 0) / max(count, 1)\n",
    "    elif edit_type == 'sub':\n",
    "        key = (x + y)[0:2]\n",
    "        count = corpus.count(y)\n",
    "        return matrices['sub'].get(key, 0) / max(count, 1)\n",
    "    elif edit_type == 'rev':\n",
    "        count = corpus.count(x + y)\n",
    "        return matrices['rev'].get(x + y, 0) / max(count, 1)\n",
    "    elif edit_type == 'del':\n",
    "        count = corpus.count(x + y)\n",
    "        return matrices['del'].get(x + y, 0) / max(count, 1)\n",
    "    return 0.0\n",
    "\n",
    "# Demo: Calculate channel probabilities\n",
    "corpus_str = ' '.join(words)\n",
    "prob_del = channel_model_probability('t', 'tr', 'del', matrices, corpus_str)\n",
    "print(f\"P(deleting 't' from 'tr'): {prob_del:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8993e785",
   "metadata": {},
   "source": [
    "## 12. Word Correction with Noisy Channel Model\n",
    "\n",
    "Combine channel model and language model to pick the best correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11167abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_word(word, prev_word, next_word, context):\n",
    "    \"\"\"Correct a single word using noisy channel model.\"\"\"\n",
    "    candidates = generate_candidates(word, context['words'])\n",
    "    \n",
    "    if word in candidates:\n",
    "        return word\n",
    "    \n",
    "    scores = {}\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        edit_info = detect_edit_type(candidate, word)\n",
    "        \n",
    "        if edit_info[0] == \"None\":\n",
    "            continue\n",
    "        \n",
    "        edit_type_map = {\n",
    "            \"Insertion\": ('add', edit_info[3][0] if len(edit_info[3]) > 0 else '', \n",
    "                         edit_info[3][1] if len(edit_info[3]) > 1 else ''),\n",
    "            \"Deletion\": ('del', edit_info[4][0] if len(edit_info[4]) > 0 else '', \n",
    "                        edit_info[4][1] if len(edit_info[4]) > 1 else ''),\n",
    "            \"Reversal\": ('rev', edit_info[4][0] if len(edit_info[4]) > 0 else '', \n",
    "                        edit_info[4][1] if len(edit_info[4]) > 1 else ''),\n",
    "            \"Substitution\": ('sub', edit_info[3], edit_info[4])\n",
    "        }\n",
    "        \n",
    "        if edit_info[0] not in edit_type_map:\n",
    "            continue\n",
    "        \n",
    "        edit_type, x, y = edit_type_map[edit_info[0]]\n",
    "        channel_prob = channel_model_probability(x, y, edit_type, \n",
    "                                                 context['matrices'], context['corpus'])\n",
    "        \n",
    "        if next_word:\n",
    "            phrase = f\"{prev_word} {candidate} {next_word}\" if prev_word else f\"{candidate} {next_word}\"\n",
    "        else:\n",
    "            phrase = f\"{prev_word} {candidate}\" if prev_word else candidate\n",
    "        \n",
    "        try:\n",
    "            lm_prob = math.exp(sentence_probability(phrase, context['models'], 'bi', 'log'))\n",
    "        except:\n",
    "            lm_prob = 1e-10\n",
    "        \n",
    "        scores[candidate] = channel_prob * lm_prob * 1e9\n",
    "    \n",
    "    if scores:\n",
    "        return max(scores, key=scores.get)\n",
    "    return ''\n",
    "\n",
    "# Demo: Prepare context\n",
    "context = {\n",
    "    'models': models,\n",
    "    'words': vocab,\n",
    "    'matrices': matrices,\n",
    "    'corpus': corpus_str\n",
    "}\n",
    "\n",
    "# Test word correction\n",
    "corrected = correct_word('acress', 'a', '', context)\n",
    "print(f\"'acress' → '{corrected}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81504838",
   "metadata": {},
   "source": [
    "## 13. Sentence Correction\n",
    "\n",
    "Now let's correct an entire sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e73abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_sentence(sentence, context):\n",
    "    \"\"\"Correct spelling errors in a sentence.\"\"\"\n",
    "    words = sentence.lower().split()\n",
    "    corrected = []\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        prev_word = words[i-1] if i > 0 else ''\n",
    "        next_word = words[i+1] if i < len(words) - 1 else ''\n",
    "        \n",
    "        corrected_word = correct_word(word, prev_word, next_word, context)\n",
    "        corrected.append(corrected_word if corrected_word else word)\n",
    "    \n",
    "    return ' '.join(corrected)\n",
    "\n",
    "# Demo: Correct the example sentence\n",
    "test_sentence = \"she is a briliant acress\"\n",
    "corrected_sentence = correct_sentence(test_sentence, context)\n",
    "\n",
    "print(f\"Original:  '{test_sentence}'\")\n",
    "print(f\"Corrected: '{corrected_sentence}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69297b05",
   "metadata": {},
   "source": [
    "## 14. Additional Test Cases\n",
    "\n",
    "Let's try a few more examples to see how well it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b049c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    \"she is a briliant acress\",\n",
    "    \"the acotr was gerat\",\n",
    "    \"hold your horeses\",\n",
    "]\n",
    "\n",
    "print(\"Spell Correction Results:\")\n",
    "print(\"=\" * 60)\n",
    "for test in test_cases:\n",
    "    corrected = correct_sentence(test, context)\n",
    "    print(f\"Input:  {test}\")\n",
    "    print(f\"Output: {corrected}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b9f0b0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This functional approach to spell correction combines:\n",
    "\n",
    "1. **N-gram Language Models**: Predict likely word sequences\n",
    "2. **Damerau-Levenshtein Distance**: Find words within 1 edit\n",
    "3. **Confusion Matrices**: Model common typing errors\n",
    "4. **Noisy Channel Model**: Combine error probability × language model probability\n",
    "\n",
    "The result is a powerful spell checker that considers both the likelihood of the typo and the linguistic context!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
